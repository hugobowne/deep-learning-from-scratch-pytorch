{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning objectives of the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Extend the ideas developed in the previous notebook to build a neural network using functions;\n",
    "- Implement functions for the initialization of, forward propagation through, and updating of a neural network;\n",
    "- Apply the logic of backpropagation in more detail;\n",
    "- Hand-code gradient descent using `numpy` for a more realistic size of problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having spent spent some time working on the ideas of supervised learning and getting familiar with the terminology of neural networks, let's write some code to implement a neural network from scratch. We're going to use a functional programming style to help build intuition. To make matters easier, we'll use a dictionary called `model` to store all data associated with the neural network (the weight matrices, the  bias vectors, etc.) and pass that into functions as a single argument. We'll also assume that the activation functions are the same in all the layers (i.e., the *logistic* or *sigmoid* function) to simplify the implementation. Production codes usually use an object-oriented style to build networks and, of course, are optimized for efficiency (unlike what we'll develop here)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to borrow notation from Michael Neilsen's [*Neural Networks and Deep Learning*](http://neuralnetworksanddeeplearning.com) to make life easier. In particular, we'll let $W^\\ell$ and $b^\\ell$ denote the weight matrices & bias vectors respectively associated with the $\\ell$th layer of the network. The entry $W^{\\ell}_{jk}$ of $W^\\ell$ is the weight parameter associated with the link connecting the $k$th neuron in layer $\\ell-1$ to the $j$th neuron in layer $\\ell$:\n",
    "\n",
    "[![](../img/tikz16.png)](http://neuralnetworksanddeeplearning.com/chap2.html)\n",
    "\n",
    "Let's put this altogether now and construct a network from scratch. We start with some typical imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create an initialization function to set up model\n",
    "\n",
    "Rather than the fixed constants in `setup` from before, write a function `initialize_model` that accepts a list  `dimensions` of positive integer inputs that constructs a `dict` with specific key-value pairs:\n",
    "+ `model['nlayers']` : number of layers in neural network\n",
    "+ `model['weights']` : list of NumPy matrices with appropriate dimensions\n",
    "+ `model['biases']` : list of NumPy (column) vectors of appropriate dimensions\n",
    "+ The matrices in `model['weights']` and the vectors in `model['biases']` should be initialized as randomly arrays of the appropriate shapes.\n",
    "\n",
    "If the input list `dimensions` has `L+1` entries, the number of layers is `L` (the first entry of `dimensions` is the input dimension, the next ones are the number of units/neurons in each subsequent layer going up to the output layer).\n",
    "Thus, for example:\n",
    "\n",
    "```python\n",
    ">>> dimensions = [784, 15, 10]\n",
    ">>> model = initialize_model(dimensions)\n",
    ">>> for k, (W, b) in enumerate(zip(model['weights'], model['biases'])):\n",
    ">>>    print(f'Layer {k+1}:\\tShape of W{k+1}: {W.shape}\\tShape of b{k+1}: {b.shape}')\n",
    "```\n",
    "```\n",
    "Layer 1:\tShape of W1: (15, 784)\tShape of b1: (15, 1)\n",
    "Layer 2:\tShape of W2: (10, 15)\tShape of b2: (10, 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(dimensions):\n",
    "    '''Accepts a list of positive integers; returns a dict 'model' with key/values as follows:\n",
    "      model['nlayers'] : number of layers in neural network\n",
    "      model['weights'] : list of NumPy matrices with appropriate dimensions\n",
    "      model['biases'] : list of NumPy (column) vectors of appropriate dimensions\n",
    "    These correspond to the weight matrices & bias vectors associated with each layer of a neural network.'''\n",
    "    weights, biases = [], []\n",
    "    L = len(dimensions) - 1 # number of layers (i.e., excludes input layer)\n",
    "    for l in range(L):\n",
    "        W = np.random.randn(dimensions[l+1], dimensions[l])\n",
    "        b = np.random.randn(dimensions[l+1], 1)\n",
    "        weights.append(W)\n",
    "        biases.append(b)\n",
    "    return dict(weights=weights, biases=biases, nlayers=L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a test example to illustrate that the network is initialized as needed\n",
    "dimensions = [784, 15, 10]\n",
    "model = initialize_model(dimensions)\n",
    "for k, (W, b) in enumerate(zip(model['weights'], model['biases'])):\n",
    "    print(f'Layer {k+1}:\\tShape of W{k+1}: {W.shape}\\tShape of b{k+1}: {b.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine the weight matrix & bias vector associated with the second layer.\n",
    "print(f'W2:\\n\\n{model[\"weights\"][1]}')  # Expect a 10x15 matrix of random numbers\n",
    "print(f'b2:\\n\\n{model[\"biases\"][1]}')   # Expect a 10x1 vector of random numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implement activation function(s), loss functions, & their derivatives\n",
    "For today's purposes, we'll use only the *logistic* or *sigmoid* function as an activation function:\n",
    "$$ \\sigma(x) = \\frac{1}{1+\\exp(-x)} = \\frac{\\exp(x)}{1+\\exp(x)}.$$\n",
    "A bit of calculus shows that\n",
    "$$ \\sigma'(x) = \\sigma(x)(1-\\sigma(x)) .$$\n",
    "\n",
    "Actually, a more numerically robust formula for $\\sigma(x)$ (i.e., one that works for large positive or large negative input equally well) is\n",
    "$$\n",
    "\\sigma(x) = \\begin{cases} \\frac{1}{1+\\exp(-x)} & (x\\ge0) \\\\ 1 - \\frac{1}{1+\\exp(x)} & \\mathrm{otherwise} \\end{cases}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the loss function, we'll use the typical \"$L_2$-norm of the error\" (alternatively called *mean-square error (MSE)* when averaged over a batch of values:\n",
    "$$ \\mathcal{E}(\\hat{y},y) = \\frac{1}{2} \\|\\hat{y}-y\\|^{2} = \\frac{1}{2} \\sum_{k=1}^{d} \\left[ \\hat{y}_{k}-y_{k} \\right]^{2}.$$\n",
    "Again, using multivariable calculus, we can see that\n",
    "$$\\nabla_{\\hat{y}} \\mathcal{E}(\\hat{y},y) = \\hat{y} - y.$$\n",
    "\n",
    "Implement all four of these functions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma(x):\n",
    "    '''The logistic function; accepts arbitrary arrays as input (vectorized)'''\n",
    "    return np.where(x>=0, 1/(1+np.exp(-x)), 1 - 1/(1+np.exp(x))) # piecewise for numerical robustness\n",
    "def sigma_prime(x):\n",
    "    '''The *derivative* of the logistic function; accepts arbitrary arrays as input (vectorized)'''\n",
    "    return sigma(x)*(1-sigma(x)) # Derivative of logistic function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(yhat, y):\n",
    "    '''The loss as measured by the L2-norm squared of the error'''\n",
    "    return 0.5 * np.square(yhat-y).sum()\n",
    "def loss_prime(yhat, y):\n",
    "    '''Implementation of the gradient of the loss function'''\n",
    "    return (yhat - y) # gradient w.r.t. yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implement a function for forward propagation\n",
    "\n",
    "Write a function `forward` that uses the architecture described in a `dict` as created by `initialize_model` to evaluate the output of the neural network for a given input *column* vector `x`.\n",
    "+ Take $a^{0}=x$ from the input.\n",
    "+ For $\\ell=1,\\dotsc,L$, compute & store the intermediate computed vectors $z^{\\ell}=W^{\\ell}a^{\\ell-1}+b^{\\ell}$ (the *weighted inputs*) and $a^{\\ell}=\\sigma\\left(z^{\\ell}\\right)$ (the *activations*) in an updated dictionary `model`. That is, modify the input dictionary `model` so as to accumulate:\n",
    "  + `model['activations']`: a list with entries $a^{\\ell}$ for $\\ell=0,\\dotsc,L$\n",
    "  + `model['z_inputs']`: a list with entries $z^{\\ell}$ for $\\ell=1,\\dotsc,L$\n",
    "+ The function should return the computed output $a^{L}$ and the modified dictionary `model`.\n",
    "Notice that input `z` can be a matrix of dimension $n_{0} \\times N_{\\mathrm{batch}}$ corresponding to a batch of input vectors (here, $n_0$ is the dimension of the expected input vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abstract process into function and run tests again.\n",
    "def forward(x, model):\n",
    "    '''Implementation of forward propagation through a feed-forward neural network.\n",
    "       x : input array oriented column-wise (i.e., features along the rows)\n",
    "       model : dict with same keys as output of initialize_model & appropriate lists in 'weights' & 'biases'\n",
    "    The output dict model is the same as the input with additional keys 'z_inputs' & 'activations';\n",
    "    these are accumulated to be used later for backpropagation. Notice the lists model['z_inputs'] &\n",
    "    model['activations'] both have the same number of entries as model['weights'] & model['biases']\n",
    "    (one for each layer).\n",
    "    '''\n",
    "    a = x\n",
    "    activations = [a]\n",
    "    zs = []\n",
    "    for W, b in zip(model['weights'], model['biases']):\n",
    "        z = W @ a + b\n",
    "        a = sigma(z)\n",
    "        zs.append(z)\n",
    "        activations.append(a)\n",
    "    model['activations'], model['z_inputs'] = activations, zs\n",
    "    return (a, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a test example to illustrate that the network is initialized as needed\n",
    "dimensions = [784, 15, 10]\n",
    "model = initialize_model(dimensions)\n",
    "print(f'Before executing *forward*:\\nkeys == {model.keys()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_batch = 3  # Let's use, say, 3 random inputs & their corresponding outputs\n",
    "x_input = np.random.rand(dimensions[0], N_batch)\n",
    "y = np.random.rand(dimensions[-1], N_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat, model = forward(x_input, model)  # the dict model is *updated* by forward propagation\n",
    "print(f'After executing *forward*:\\nkeys == {model.keys()}')\n",
    "# Observe additional dict keys: 'activations' & 'z_inputs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm for backpropagation:\n",
    "\n",
    "#### (optional reading for the mathematically brave)\n",
    "\n",
    "The description here is based on the *wonderfully concise* description from Michael Neilsen's [*Neural Networks and Deep Learning*](http://neuralnetworksanddeeplearning.com/chap2.html). Neilsen has artfully crafted a summary using the bare minimum mathematical prerequisites. The notation elegantly summarises the important ideas in a way to make implementation easy in array-based frameworks like Matlab or NumPy. This is the best description I (Dhavide) know of that does this.\n",
    "\n",
    "In the following, $\\mathcal{E}$ is the loss function and the symbol $\\odot$ is the [*Hadamard product*](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)) of two conforming arrays; this is simply a fancy way of writing the usual element-wise product of arrays as computed by NumPy & is sometimes called the *Schur product*. This can be reformulated in usual matrix algebra for analysis.\n",
    "\n",
    "Given a neural network with $L$ layers (not including the \"input layer\") described by an appropriate architecture:\n",
    "\n",
    "1. Input $x$: Set the corresponding activation $a^{0} \\leftarrow x$ for the input layer.\n",
    "2. Feedforward: For each $\\ell=1,2,\\dotsc,L$, compute *weighted inputs* $z^{\\ell}$ & *activations* $a^{\\ell}$ using the formulas\n",
    "$$\n",
    "\\begin{aligned}\n",
    "z^{\\ell} & \\leftarrow  W^{\\ell} a^{\\ell-1} + b^{\\ell}, \\\\\n",
    "a^{\\ell} & \\leftarrow  \\sigma\\left( z^{\\ell}\\right)\n",
    "\\end{aligned}.\n",
    "$$\n",
    "3. Starting from the end, compute the \"error\" in the output layer $\\delta^{L}$ according to the formula\n",
    "$$\n",
    "\\delta^{L} \\leftarrow \\nabla_{a^{L}} \\mathcal{E} \\odot \\sigma'\\left(z^{L}\\right)\n",
    "$$\n",
    "\n",
    "4. *Backpropagate* the \"error\" for $\\ell=Lâˆ’1\\dotsc,1$ using the formula\n",
    "$$\n",
    "\\delta^{\\ell} \\leftarrow \\left[ W^{\\ell+1}\\right]^{T}\\delta^{\\ell+1} \\odot \\sigma'\\left(z^{\\ell}\\right).\n",
    "$$\n",
    "5. The required gradients of the loss function $\\mathcal{E}$ with respect to the parameters $W^{\\ell}_{p,q}$ and $b^{\\ell}_{r}$ can be computed directly from the \"errors\" $\\left\\{ \\delta^{\\ell} \\right\\}$ and the weighted inputs $\\left\\{ z^{\\ell} \\right\\}$ according to the relations\n",
    "$$\n",
    "\\begin{aligned}\n",
    "   \\frac{\\partial \\mathcal{E}}{\\partial W^{\\ell}_{p,q}} &= a^{\\ell-1}_{q} \\delta^{\\ell}_{p} &&(\\ell=1,\\dotsc,L)\\\\\n",
    "   \\frac{\\partial \\mathcal{E}}{\\partial b^{\\ell}_{r}} &= \\delta^{\\ell}_{r} &&\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implement a function for backward propagation\n",
    "\n",
    "**This one is a freebie!**\n",
    "\n",
    "Implement a function `backward` that implements the back-propagation algorithm to compute the gradients of the loss function $\\mathcal{E}$ with respect to the weight matrices $W^{\\ell}$ and the bias vectors $b^{\\ell}$.\n",
    "+ The function should accept a column vector `y` of output labels and an appropriate dictionary `model` as input.\n",
    "+ The dict `model` is assumed to have been generated *after* a call to `forward`; that is, `model` should have keys `'w_inputs'` and `'activations'` as computed by a call to `forward`.\n",
    "+ The result will be a modified dictionary `model` with two additional key-value pairs:\n",
    "  + `model['grad_weights']`: a list with entries $\\nabla_{W^{\\ell}} \\mathcal{E}$ for $\\ell=1,\\dotsc,L$\n",
    "  + `model['grad_biases']`: a list with entries $\\nabla_{b^{\\ell}} \\mathcal{E}$ for $\\ell=1,\\dotsc,L$\n",
    "+ Notice the dimensions of the matrices $\\nabla_{W^{\\ell}} \\mathcal{E}$ and the vectors $\\nabla_{b^{\\ell}} \\mathcal{E}$ will be identical to those of ${W^{\\ell}}$ and ${b^{\\ell}}$ respectively.\n",
    "+ The function's return value is the modified dictionary `model`.\n",
    "\n",
    "\n",
    "We've done this for you (in the interest of time). Notice that input `y` can be a matrix of dimension $n_{L} \\times N_{\\mathrm{batch}}$ corresponding to a batch of output vectors (here, $n_L$ is the number of units in the output layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(y, model):\n",
    "    '''Implementation of backward propagation of data through the network\n",
    "       y : output array oriented column-wise (i.e., features along the rows) as output by forward\n",
    "       model : dict with same keys as output by forward\n",
    "    Note the input needs to have keys 'nlayers', 'weights', 'biases', 'z_inputs', and 'activations'\n",
    "    '''\n",
    "    Nbatch = y.shape[1] # Needed to extend for batches of vectors\n",
    "    # Compute the \"error\" delta^L for the output layer\n",
    "    yhat = model['activations'][-1]\n",
    "    z, a = model['z_inputs'][-1], model['activations'][-2]\n",
    "    delta = loss_prime(yhat, y) * sigma_prime(z)\n",
    "    # Use delta^L to compute gradients w.r.t b & W in the output layer.\n",
    "    grad_b, grad_W = delta @ np.ones((Nbatch, 1)), np.dot(delta, a.T)\n",
    "    grad_weights, grad_biases = [grad_W], [grad_b]\n",
    "    loop_iterates = zip(model['weights'][-1:0:-1],\n",
    "                        model['z_inputs'][-2::-1],\n",
    "                        model['activations'][-3::-1])\n",
    "    for W, z, a in loop_iterates:\n",
    "        delta = np.dot(W.T, delta) * sigma_prime(z)\n",
    "        grad_b, grad_W = delta @ np.ones((Nbatch, 1)), np.dot(delta, a.T)\n",
    "        grad_weights.append(grad_W)\n",
    "        grad_biases.append(grad_b)\n",
    "    # We built up lists of gradients backwards, so we reverse the lists\n",
    "    model['grad_weights'], model['grad_biases'] = grad_weights[::-1], grad_biases[::-1]\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the test example from above. Assume model, x_input have been initialized & *forward* has been executed already.\n",
    "print(f'Before executing *backward*:\\nkeys == {model.keys()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = backward(y, model)  # the dict model is updated *again* by backward propagation\n",
    "print(f'After executing *backward*:\\nkeys == {model.keys()}')\n",
    "# Observe additional dict keys: 'grad_weights' & 'grad_biases'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Implement a function to update the model parameters using computed gradients.\n",
    "\n",
    "Given some positive learning rate $\\eta>0$, we want to change all the weights and biases using their gradients.\n",
    "Write a function `update` to compute a single step of gradient descent assuming that the model gradients have been computed for a given input vector.\n",
    "+ The functions signature should be `update(eta, model)` where `eta` is a positive scalar value and `model` is a dictionary as output from `backward`.\n",
    "+ The result will be an updated model with the values updated for `model['weights']` and `model['biases']`.\n",
    "+ Written using array notations, these updates can be expressed as\n",
    "   $$\n",
    "   \\begin{aligned}\n",
    "   W^{\\ell} &\\leftarrow W^{\\ell} - \\eta \\nabla_{W^{\\ell}} \\mathcal{E} &&(\\ell=1,\\dotsc,L)\\\\\n",
    "   b^{\\ell} &\\leftarrow b^{\\ell} - \\eta \\nabla_{b^{\\ell}} \\mathcal{E} &&\n",
    "   \\end{aligned}.\n",
    "   $$\n",
    "+ Written out component-wise, the preceding array expressions would be written as\n",
    "   $$\n",
    "   \\begin{aligned}\n",
    "      W^{\\ell}_{p,q} &\\leftarrow W^{\\ell}_{p,q} - \\eta \\frac{\\partial \\mathcal{E}}{\\partial W^{\\ell}_{p,q}}\n",
    "      &&(\\ell=1,\\dotsc,L)\\\\\n",
    "      b^{\\ell}_{r} &\\leftarrow b^{\\ell}_{r} - \\eta \\frac{\\partial \\mathcal{E}}{\\partial b^{\\ell}_{r}} &&\n",
    "   \\end{aligned}\n",
    "   $$.\n",
    "+ For safety, have the update step delete the keys added by calls to `forward` and `backward`, i.e., the keys `'z_inputs'`, `'activations'`, `'grad_weights'`, & `'grad_biases'`.\n",
    "+ The output should be a dict `model` like before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(eta, model):\n",
    "    '''Use learning rate and gradients to update model parameters\n",
    "       eta : learning rate (positive scalar parameter)\n",
    "       model : dict with same keys as output by backward\n",
    "    Output result is a modified dict model\n",
    "    '''\n",
    "    new_weights, new_biases = [], []\n",
    "    for W, b, dW, db in zip(model['weights'], model['biases'], model['grad_weights'], model['grad_biases']):\n",
    "        new_weights.append(W - (eta * dW))\n",
    "        new_biases.append(b- (eta * db))\n",
    "    model['weights'] = new_weights\n",
    "    model['biases'] = new_biases\n",
    "    # Get rid of extraneous keys/values\n",
    "    for key in ['z_inputs', 'activations', 'grad_weights', 'grad_biases']:\n",
    "        del model[key]\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the test example from above. Assume *forward* & *backward* have been executed already.\n",
    "print(f'Before executing *update*:\\nkeys == {model.keys()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.5  # Choice of learning rate\n",
    "model = update(eta, model)  # the dict model is updated *again* by calling *update*\n",
    "print(f'After executing *update*:\\nkeys == {model.keys()}')\n",
    "# Observe fewer dict keys: extraneous keys have been freed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe the required sequence of executions: (forward -> backward -> update -> forward -> backward -> ...)\n",
    "# If done out of sequence, results in KeyError\n",
    "backward(y, model)  # This should cause an exception (KeyError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Implement steepest descent in a loop for random training data\n",
    "\n",
    "Let's now attempt to use our NumPy-based model to implement the steepest descent algorithm. We'll explain these numbers shortly in the context of the MNIST digit classification problem.\n",
    "\n",
    "+ Generate random arrays `X` and `y` of dimensions $28^2 \\times N_{\\mathrm{batch}}$ and $10\\times N_{\\mathrm{batch}}$ respectively where $N_{\\mathrm{batch}}=10$.\n",
    "+ Initialize the network architecture using `initialize_model` as above to require an input layer of $28^2$ units, a hidden layer of 15 units, and an output layer of 10 units.\n",
    "+ Choose a learning rate of, say, $\\eta=0.5$ and a number of epochs `n_epoch` of, say, $30$.\n",
    "+ Construct a for loop with `n_epochs` iterations in which:\n",
    "    + The output `yhat` is computed from the input`X` using `forward`.\n",
    "    + The function `backward` is called to compute the gradients of the loss function with respect to the weights and biases.\n",
    "    + Update the network parameters using the function `update`.\n",
    "    + Compute and print out the epoch (iteration counter) and the value of the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_batch = 10\n",
    "n_epochs = 30\n",
    "dimensions = [784, 15, 10]\n",
    "X = np.random.randn(dimensions[0], N_batch)\n",
    "y = np.random.randn(dimensions[-1], N_batch)\n",
    "eta = 0.5\n",
    "model = initialize_model(dimensions)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    yhat, model = forward(X, model)\n",
    "    err = loss(yhat, y)\n",
    "    print(f'Epoch: {epoch}\\tLoss: {err}')\n",
    "    model = backward(y, model)\n",
    "    model = update(eta, model)\n",
    "\n",
    "# Expect to see loss values decreasing systematically in each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Modify the steepest descent loop to make a plot\n",
    "\n",
    "Let's alter the preceding loop to accumulate selected epoch & loss values in lists for plotting.\n",
    "\n",
    "+ Set `N_batch` and `n_epochs` to be larger, say, $50$ and $30,000$ respectively.\n",
    "+ Change the preceding `for` loop so that:\n",
    "    + The `epoch` counter and the loss value are accumulated into lists every, say, `SKIP` iterations where `SKIP==500`.\n",
    "    + Eliminate the `print` statement(s) to save on output.\n",
    "+ After the `for` loop terminates, make a `semilogy` plot to verify that the loss function is actually decreasing with sucessive epochs.\n",
    "    + Use the list `epochs` to accumulate the `epoch` every 500 epochs.\n",
    "    + Use the list `losses` to accumulate the values of the loss function every 500 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_batch = 50\n",
    "n_epochs = 30000\n",
    "SKIP = 50\n",
    "dimensions = [784, 15, 10]\n",
    "X = np.random.randn(dimensions[0], N_batch)\n",
    "y = np.random.randn(dimensions[-1], N_batch)\n",
    "eta = 0.5\n",
    "model = initialize_model(dimensions)\n",
    "\n",
    "# accumulate the epoch and loss in these respective lists\n",
    "epochs, losses = [], []\n",
    "for epoch in range(n_epochs):\n",
    "    yhat, model = forward(X, model)\n",
    "    model = backward(y, model)\n",
    "    model = update(eta, model)\n",
    "    if (divmod(epoch, SKIP)[1]==0):\n",
    "        err = loss(yhat, y)\n",
    "        epochs.append(epoch)\n",
    "        losses.append(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for plotting once that the lists epochs and losses are accumulated\n",
    "fig = plt.figure(); ax = fig.add_subplot(111)\n",
    "ax.set_xlim([0,n_epochs]); ax.set_ylim([min(losses), max(losses)]);\n",
    "ax.set_xticks(epochs[::500]); ax.set_xlabel(\"Epochs\"); ax.grid(True);\n",
    "ax.set_ylabel(r'$\\mathcal{E}$'); \n",
    "h1 = ax.semilogy(epochs, losses, 'r-', label=r'$\\mathcal{E}$')\n",
    "plt.title('Loss vs. epochs');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
